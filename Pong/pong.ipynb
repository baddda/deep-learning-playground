{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Based on https://blog.floydhub.com/spinning-up-with-deep-reinforcement-learning/\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from datetime import datetime\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect a game of Pong with random input.\n",
    "frames = []\n",
    "frames_ram = []\n",
    "STEPS = 300\n",
    "\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "env = gym.make(\"Pong-ram-v0\")\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(STEPS):\n",
    "    gym.envs.registry\n",
    "    action = random.randint(UP_ACTION, DOWN_ACTION)\n",
    "\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    frames.append(env.unwrapped._get_image())\n",
    "    frames_ram.append(env.unwrapped._get_ram())\n",
    "    \n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "        frames.append(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i in range(len(frames)):\\n    plt.imshow(frames[i])\\n    print(frames_ram[i])\\n    print('Player position y: ' + str(frames_ram[i][ram_address_position_player_y]))\\n    print('Ball position x: ' + str(frames_ram[i][ram_address_position_ball_x]))\\n    print('Ball position y: ' + str(frames_ram[i][ram_address_position_ball_y]))\\n    display.display(pl.gcf())\\n    time.sleep(0.5)\\n    display.clear_output(wait=True)\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw our frames to see what happened and print ram to find important addresses.\n",
    "ram_address_position_player_y = 60\n",
    "ram_address_position_ball_x = 49\n",
    "ram_address_position_ball_y = 50\n",
    "\n",
    "'''\n",
    "for i in range(len(frames)):\n",
    "    plt.imshow(frames[i])\n",
    "    print(frames_ram[i])\n",
    "    print('Player position y: ' + str(frames_ram[i][ram_address_position_player_y]))\n",
    "    print('Ball position x: ' + str(frames_ram[i][ram_address_position_ball_x]))\n",
    "    print('Ball position y: ' + str(frames_ram[i][ram_address_position_ball_y]))\n",
    "    display.display(pl.gcf())\n",
    "    time.sleep(0.5)\n",
    "    display.clear_output(wait=True)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network.\n",
    "# TODO: What is a adam optimizer?\n",
    "model = Sequential()\n",
    "model.add(Dense(units=16,input_dim=6, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "model.add(Dense(units=1, activation='sigmoid', kernel_initializer='RandomNormal'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# gym initialization\n",
    "env = gym.make(\"Pong-ram-v0\")\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "\n",
    "# Macros\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "\n",
    "# initialization of variables used in the main loop\n",
    "x_train, y_train, rewards = [],[],[]\n",
    "reward_sum = 0\n",
    "episode_nb = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './log' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \"/\"\n",
    "resume = True\n",
    "epochs_before_saving = 100\n",
    "\n",
    "# load pre-trained model if exist\n",
    "if (resume and os.path.isfile('my_model_weights.h5')):\n",
    "    print(\"loading previous weights\")\n",
    "    model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    r = np.array(r)\n",
    "    #print('discount_rewards r:' + str(r))\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # we go from last reward to first one so we don't have to do exponentiations\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
    "        running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
    "        discounted_r[t] = running_add\n",
    "    discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "    discounted_r /= np.std(discounted_r) #idem\n",
    "    #print('discount_rewards discounted_r:' + str(discounted_r))\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At the end of episode 0 the total reward was : -21.0\n",
      "Average of current run: -21.0\n",
      "Epoch 1/1\n",
      "1179/1179 [==============================] - 0s 133us/step - loss: -1.5214e-04 - accuracy: 0.5072\n",
      "At the end of episode 1 the total reward was : -21.0\n",
      "Average of current run: -21.0\n",
      "Epoch 1/1\n",
      "1199/1199 [==============================] - 0s 35us/step - loss: 0.0015 - accuracy: 0.5463\n",
      "At the end of episode 2 the total reward was : -21.0\n",
      "Average of current run: -21.0\n",
      "Epoch 1/1\n",
      "1522/1522 [==============================] - 0s 33us/step - loss: -0.0022 - accuracy: 0.5302\n",
      "At the end of episode 3 the total reward was : -20.0\n",
      "Average of current run: -20.75\n",
      "Epoch 1/1\n",
      "1308/1308 [==============================] - 0s 29us/step - loss: -0.0026 - accuracy: 0.5191\n",
      "At the end of episode 4 the total reward was : -20.0\n",
      "Average of current run: -20.6\n",
      "Epoch 1/1\n",
      "1231/1231 [==============================] - 0s 28us/step - loss: 0.0029 - accuracy: 0.5361\n",
      "At the end of episode 5 the total reward was : -21.0\n",
      "Average of current run: -20.666666666666668\n",
      "Epoch 1/1\n",
      "1171/1171 [==============================] - 0s 34us/step - loss: -3.7511e-04 - accuracy: 0.5337\n",
      "At the end of episode 6 the total reward was : -20.0\n",
      "Average of current run: -20.571428571428573\n",
      "Epoch 1/1\n",
      "1634/1634 [==============================] - 0s 27us/step - loss: -2.2500e-04 - accuracy: 0.5318\n",
      "At the end of episode 7 the total reward was : -18.0\n",
      "Average of current run: -20.25\n",
      "Epoch 1/1\n",
      "1568/1568 [==============================] - 0s 26us/step - loss: -7.2634e-04 - accuracy: 0.5121\n",
      "At the end of episode 8 the total reward was : -21.0\n",
      "Average of current run: -20.333333333333332\n",
      "Epoch 1/1\n",
      "1176/1176 [==============================] - 0s 26us/step - loss: 0.0014 - accuracy: 0.5187\n",
      "At the end of episode 9 the total reward was : -20.0\n",
      "Average of current run: -20.3\n",
      "Epoch 1/1\n",
      "1213/1213 [==============================] - 0s 26us/step - loss: 3.0409e-04 - accuracy: 0.5515\n",
      "At the end of episode 10 the total reward was : -20.0\n",
      "Average of current run: -20.272727272727273\n",
      "Epoch 1/1\n",
      "1398/1398 [==============================] - 0s 27us/step - loss: 0.0040 - accuracy: 0.5243\n",
      "At the end of episode 11 the total reward was : -19.0\n",
      "Average of current run: -20.166666666666668\n",
      "Epoch 1/1\n",
      "1503/1503 [==============================] - 0s 26us/step - loss: -0.0011 - accuracy: 0.4970\n",
      "At the end of episode 12 the total reward was : -21.0\n",
      "Average of current run: -20.23076923076923\n",
      "Epoch 1/1\n",
      "1020/1020 [==============================] - 0s 29us/step - loss: 5.9688e-04 - accuracy: 0.5137\n",
      "At the end of episode 13 the total reward was : -20.0\n",
      "Average of current run: -20.214285714285715\n",
      "Epoch 1/1\n",
      "1557/1557 [==============================] - 0s 27us/step - loss: -0.0034 - accuracy: 0.5318\n",
      "At the end of episode 14 the total reward was : -21.0\n",
      "Average of current run: -20.266666666666666\n",
      "Epoch 1/1\n",
      "1437/1437 [==============================] - 0s 27us/step - loss: 5.1204e-04 - accuracy: 0.5379\n",
      "At the end of episode 15 the total reward was : -21.0\n",
      "Average of current run: -20.3125\n",
      "Epoch 1/1\n",
      "1420/1420 [==============================] - 0s 28us/step - loss: -5.3544e-04 - accuracy: 0.5176\n",
      "At the end of episode 16 the total reward was : -20.0\n",
      "Average of current run: -20.294117647058822\n",
      "Epoch 1/1\n",
      "1250/1250 [==============================] - 0s 29us/step - loss: 0.0017 - accuracy: 0.5408\n",
      "At the end of episode 17 the total reward was : -19.0\n",
      "Average of current run: -20.22222222222222\n",
      "Epoch 1/1\n",
      "1919/1919 [==============================] - 0s 24us/step - loss: 7.7948e-04 - accuracy: 0.5492\n",
      "At the end of episode 18 the total reward was : -21.0\n",
      "Average of current run: -20.263157894736842\n",
      "Epoch 1/1\n",
      "1336/1336 [==============================] - 0s 26us/step - loss: -0.0031 - accuracy: 0.5142\n",
      "At the end of episode 19 the total reward was : -21.0\n",
      "Average of current run: -20.3\n",
      "Epoch 1/1\n",
      "1429/1429 [==============================] - 0s 27us/step - loss: 2.2103e-04 - accuracy: 0.5486\n",
      "At the end of episode 20 the total reward was : -21.0\n",
      "Average of current run: -20.333333333333332\n",
      "Epoch 1/1\n",
      "1347/1347 [==============================] - 0s 28us/step - loss: -0.0031 - accuracy: 0.5226\n",
      "At the end of episode 21 the total reward was : -20.0\n",
      "Average of current run: -20.318181818181817\n",
      "Epoch 1/1\n",
      "1721/1721 [==============================] - 0s 26us/step - loss: -0.0024 - accuracy: 0.5485\n",
      "At the end of episode 22 the total reward was : -21.0\n",
      "Average of current run: -20.347826086956523\n",
      "Epoch 1/1\n",
      "1450/1450 [==============================] - 0s 29us/step - loss: -0.0017 - accuracy: 0.5476\n",
      "At the end of episode 23 the total reward was : -21.0\n",
      "Average of current run: -20.375\n",
      "Epoch 1/1\n",
      "1362/1362 [==============================] - 0s 30us/step - loss: -0.0103 - accuracy: 0.5624\n",
      "At the end of episode 24 the total reward was : -17.0\n",
      "Average of current run: -20.24\n",
      "Epoch 1/1\n",
      "1608/1608 [==============================] - 0s 27us/step - loss: -0.0022 - accuracy: 0.5858\n",
      "At the end of episode 25 the total reward was : -21.0\n",
      "Average of current run: -20.26923076923077\n",
      "Epoch 1/1\n",
      "1259/1259 [==============================] - 0s 27us/step - loss: 0.0029 - accuracy: 0.5655\n",
      "At the end of episode 26 the total reward was : -21.0\n",
      "Average of current run: -20.296296296296298\n",
      "Epoch 1/1\n",
      "1672/1672 [==============================] - 0s 28us/step - loss: 0.0056 - accuracy: 0.5652\n",
      "At the end of episode 27 the total reward was : -21.0\n",
      "Average of current run: -20.321428571428573\n",
      "Epoch 1/1\n",
      "1506/1506 [==============================] - 0s 26us/step - loss: -6.1867e-04 - accuracy: 0.5664\n",
      "At the end of episode 28 the total reward was : -20.0\n",
      "Average of current run: -20.310344827586206\n",
      "Epoch 1/1\n",
      "1378/1378 [==============================] - 0s 28us/step - loss: 0.0037 - accuracy: 0.5835\n",
      "At the end of episode 29 the total reward was : -21.0\n",
      "Average of current run: -20.333333333333332\n",
      "Epoch 1/1\n",
      "1527/1527 [==============================] - 0s 25us/step - loss: -0.0012 - accuracy: 0.5711\n",
      "At the end of episode 30 the total reward was : -21.0\n",
      "Average of current run: -20.35483870967742\n",
      "Epoch 1/1\n",
      "1735/1735 [==============================] - 0s 26us/step - loss: 0.0041 - accuracy: 0.5718\n",
      "At the end of episode 31 the total reward was : -20.0\n",
      "Average of current run: -20.34375\n",
      "Epoch 1/1\n",
      "1622/1622 [==============================] - 0s 26us/step - loss: -6.6312e-04 - accuracy: 0.6036\n",
      "At the end of episode 32 the total reward was : -19.0\n",
      "Average of current run: -20.303030303030305\n",
      "Epoch 1/1\n",
      "1839/1839 [==============================] - 0s 26us/step - loss: 0.0012 - accuracy: 0.5710\n",
      "At the end of episode 33 the total reward was : -21.0\n",
      "Average of current run: -20.323529411764707\n",
      "Epoch 1/1\n",
      "1656/1656 [==============================] - 0s 26us/step - loss: 0.0077 - accuracy: 0.5894\n",
      "At the end of episode 34 the total reward was : -20.0\n",
      "Average of current run: -20.314285714285713\n",
      "Epoch 1/1\n",
      "1899/1899 [==============================] - 0s 29us/step - loss: 0.0079 - accuracy: 0.5835\n",
      "At the end of episode 35 the total reward was : -21.0\n",
      "Average of current run: -20.333333333333332\n",
      "Epoch 1/1\n",
      "1177/1177 [==============================] - 0s 28us/step - loss: 0.0012 - accuracy: 0.5896\n",
      "At the end of episode 36 the total reward was : -20.0\n",
      "Average of current run: -20.324324324324323\n",
      "Epoch 1/1\n",
      "1476/1476 [==============================] - 0s 29us/step - loss: -0.0061 - accuracy: 0.5752\n",
      "At the end of episode 37 the total reward was : -21.0\n",
      "Average of current run: -20.342105263157894\n",
      "Epoch 1/1\n",
      "1423/1423 [==============================] - 0s 28us/step - loss: 4.7405e-05 - accuracy: 0.5924\n",
      "At the end of episode 38 the total reward was : -17.0\n",
      "Average of current run: -20.256410256410255\n",
      "Epoch 1/1\n",
      "1950/1950 [==============================] - 0s 32us/step - loss: -0.0074 - accuracy: 0.5682\n",
      "At the end of episode 39 the total reward was : -20.0\n",
      "Average of current run: -20.25\n",
      "Epoch 1/1\n",
      "1784/1784 [==============================] - 0s 30us/step - loss: -0.0065 - accuracy: 0.5858\n",
      "At the end of episode 40 the total reward was : -19.0\n",
      "Average of current run: -20.21951219512195\n",
      "Epoch 1/1\n",
      "1905/1905 [==============================] - 0s 31us/step - loss: -0.0053 - accuracy: 0.6042\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2aa7a335ae04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# forward the policy network and sample action according to the proba distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m# print('Prediction: ' + str(proba))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# Variable proba is the probability prediction of how good UP_ACTION is for this frame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "all_reward_sums = np.array([])\n",
    "prev_position_player_y = 0\n",
    "prev_position_ball_x = 0\n",
    "prev_position_ball_y = 0\n",
    "while (True):\n",
    "    ram = env.unwrapped._get_ram()\n",
    "    position_player_y = float(ram[ram_address_position_player_y])\n",
    "    position_ball_x = float(ram[ram_address_position_ball_x])\n",
    "    position_ball_y = float(ram[ram_address_position_ball_y])\n",
    "\n",
    "    # TODO: Try to give direction as number between 0-1 (0-360Â°) instead of prev position.\n",
    "    x = np.array([position_player_y, position_ball_x, position_ball_y, prev_position_player_y, prev_position_ball_x, prev_position_ball_y])    \n",
    "    x -= 100\n",
    "    x /= 200.0\n",
    "    #print('X: ' + str(x))\n",
    "    \n",
    "    prev_position_player_y = position_player_y\n",
    "    prev_position_ball_x = position_ball_x\n",
    "    prev_position_ball_y = position_ball_y\n",
    "    \n",
    "    # TODO: Why in the simulation it gets stuck at the top? But actually it should regulate itself?\n",
    "    # Draw current state.\n",
    "    '''\n",
    "    plt.imshow(env.unwrapped._get_image())\n",
    "    print('TV:')\n",
    "    display.display(pl.gcf())\n",
    "    time.sleep(0.5)\n",
    "    display.clear_output(wait=True)\n",
    "    '''\n",
    "    \n",
    "    # forward the policy network and sample action according to the proba distribution\n",
    "    proba = model.predict(np.array([x]))\n",
    "    # print('Prediction: ' + str(proba))\n",
    "    # Variable proba is the probability prediction of how good UP_ACTION is for this frame.\n",
    "    # Then select UP_ACTION by proba percent. Easy way to still allow the other action.\n",
    "    # TODO: Mathematical reason for random number?\n",
    "    action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "    y = 1 if action == UP_ACTION else 0 # 0 and 1 are our labels\n",
    "\n",
    "    # log the input and label to train later\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "\n",
    "    # do one step in our environment\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    #print('Observation: ' + str(observation))\n",
    "    #print('Reward: ' + str(reward))\n",
    "    #print('Done: ' + str(done))\n",
    "    #print('Info: ' + str(info))\n",
    "    rewards.append(reward)\n",
    "    reward_sum += reward\n",
    "    \n",
    "    # end of an episode\n",
    "    if done:\n",
    "        print('At the end of episode', episode_nb, 'the total reward was :', reward_sum)\n",
    "        all_reward_sums = np.append(all_reward_sums, reward_sum)\n",
    "        print('Average of current run: ' + str(np.average(all_reward_sums)))\n",
    "        \n",
    "        # increment episode number\n",
    "        episode_nb += 1\n",
    "        \n",
    "        # training\n",
    "        # TODO: Is np.vstack is really necessary?\n",
    "        # TODO: Clarify the use of gamma in sample_weight=discount_rewards(rewards, gamma)\n",
    "        model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=1, sample_weight=discount_rewards(rewards, gamma))\n",
    "                                                     \n",
    "        if episode_nb % epochs_before_saving == 0:    \n",
    "            model.save_weights('my_model_weights' + datetime.now().strftime(\"%Y%m%d-%H%M%S\") + '.h5')\n",
    "            \n",
    "        # Reinitialization\n",
    "        x_train, y_train, rewards = [],[],[]\n",
    "        observation = env.reset()\n",
    "        reward_sum = 0\n",
    "        prev_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python_3_6",
   "language": "python",
   "name": "env_python_3_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
