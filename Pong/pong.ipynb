{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_34 (Dense)             (None, 3)                 15        \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 26\n",
      "Trainable params: 26\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-6ef501d24bbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# forward the policy network and sample action according to the proba distribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;31m# print('Prediction: ' + str(proba))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Variable proba is the probability prediction of how good UP_ACTION is for this frame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m         \u001b[0;31m# Case 1: generator-like. Input is Python generator, or Sequence object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_generator_or_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1426\u001b[0m             return self.predict_generator(\n\u001b[1;32m   1427\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mis_generator_or_sequence\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_generator_or_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;34m\"\"\"Check if `x` is a Keras generator type.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Kera/env_python_3_6/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mis_sequence\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[0;31m# TODO Dref360: Decide which pattern to follow. First needs a new TF Version.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     return (getattr(seq, 'use_sequence_api', False)\n\u001b[0;32m--> 607\u001b[0;31m             or set(dir(Sequence())).issubset(set(dir(seq) + ['use_sequence_api'])))\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Based on https://blog.floydhub.com/spinning-up-with-deep-reinforcement-learning/\n",
    "import gym\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from IPython import display\n",
    "from datetime import datetime\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras import regularizers\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "\n",
    "# Collect a game of Pong with random input.\n",
    "frames = []\n",
    "frames_ram = []\n",
    "STEPS = 300\n",
    "\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "env = gym.make(\"Pong-ram-v0\")\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "for i in range(STEPS):\n",
    "    gym.envs.registry\n",
    "    action = random.randint(UP_ACTION, DOWN_ACTION)\n",
    "\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    frames.append(env.unwrapped._get_image())\n",
    "    frames_ram.append(env.unwrapped._get_ram())\n",
    "    \n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "        frames.append(observation)\n",
    "\n",
    "# Draw our frames to see what happened and print ram to find important addresses.\n",
    "ram_address_position_player_y = 60\n",
    "ram_address_position_ball_x = 49\n",
    "ram_address_position_ball_y = 50\n",
    "\n",
    "'''\n",
    "for i in range(len(frames)):\n",
    "    plt.imshow(frames[i])\n",
    "    print(frames_ram[i])\n",
    "    print('Player position y: ' + str(frames_ram[i][ram_address_position_player_y]))\n",
    "    print('Ball position x: ' + str(frames_ram[i][ram_address_position_ball_x]))\n",
    "    print('Ball position y: ' + str(frames_ram[i][ram_address_position_ball_y]))\n",
    "    display.display(pl.gcf())\n",
    "    time.sleep(0.5)\n",
    "    display.clear_output(wait=True)\n",
    "'''\n",
    "\n",
    "\n",
    "# Create the neural network.\n",
    "# TODO: What is a adam optimizer?\n",
    "def get_model(configuration):\n",
    "    model = Sequential()\n",
    "    for unit_count in configuration:\n",
    "        model.add(Dense(units=unit_count,input_dim=4, activation='relu', kernel_initializer='glorot_uniform'))\n",
    "    model.add(Dense(units=1, activation='sigmoid', kernel_initializer='RandomNormal'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def discount_rewards(r, gamma):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    r = np.array(r)\n",
    "    #print('discount_rewards r:' + str(r))\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    # we go from last reward to first one so we don't have to do exponentiations\n",
    "    for t in reversed(range(0, r.size)):\n",
    "        if r[t] != 0: running_add = 0 # if the game ended (in Pong), reset the reward sum\n",
    "        running_add = running_add * gamma + r[t] # the point here is to use Horner's method to compute those rewards efficiently\n",
    "        discounted_r[t] = running_add\n",
    "    discounted_r -= np.mean(discounted_r) #normalizing the result\n",
    "    discounted_r /= np.std(discounted_r) #idem\n",
    "    #print('discount_rewards discounted_r:' + str(discounted_r))\n",
    "    return discounted_r\n",
    "\n",
    "# gym initialization\n",
    "env = gym.make(\"Pong-ram-v0\")\n",
    "observation = env.reset()\n",
    "prev_input = None\n",
    "\n",
    "# Macros\n",
    "UP_ACTION = 2\n",
    "DOWN_ACTION = 3\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "configurations = [[3,2]]\n",
    "max_episodes = 3000\n",
    "epochs_before_saving = 100\n",
    "\n",
    "# load pre-trained model if exist\n",
    "if (os.path.isfile('my_model_weights.h5')):\n",
    "    print(\"loading previous weights\")\n",
    "    model.load_weights('my_model_weights.h5')\n",
    "\n",
    "# main loop\n",
    "for configuration in configurations:\n",
    "    x_train, y_train, rewards = [],[],[]\n",
    "    reward_sum = 0\n",
    "    episode_nb = 0\n",
    "    model = get_model(configuration)\n",
    "    print(model.summary())\n",
    "    all_reward_sums = np.array([])\n",
    "    prev_position_player_y = 0\n",
    "    prev_position_ball_x = 0\n",
    "    prev_position_ball_y = 0\n",
    "    while (episode_nb < max_episodes):\n",
    "        ram = env.unwrapped._get_ram()\n",
    "        position_player_y = float(ram[ram_address_position_player_y])\n",
    "        position_ball_x = float(ram[ram_address_position_ball_x])\n",
    "        position_ball_y = float(ram[ram_address_position_ball_y])\n",
    "        \n",
    "        # TODO: normalize proper\n",
    "        direction_ball = math.atan2(position_ball_y-prev_position_player_y, position_ball_x-prev_position_ball_x)\n",
    "        #direction_ball_degrees = math.degrees(direction_ball)\n",
    "        #print('Direction of ball: ' + str(direction_ball))\n",
    "    \n",
    "        # TODO: Try to give direction as number between 0-1 (0-360Â°) instead of prev position.\n",
    "        x = np.array([position_player_y, position_ball_x, position_ball_y, direction_ball])    \n",
    "        # TODO: normalize based on real max and min values for x and y\n",
    "        x -= 100.0\n",
    "        x /= 200.0\n",
    "        #print('X: ' + str(x))\n",
    "        \n",
    "        prev_position_player_y = position_player_y\n",
    "        prev_position_ball_x = position_ball_x\n",
    "        prev_position_ball_y = position_ball_y\n",
    "        \n",
    "        # Draw current state.\n",
    "        '''\n",
    "        plt.imshow(env.unwrapped._get_image())\n",
    "        print('TV:')\n",
    "        display.display(pl.gcf())\n",
    "        time.sleep(0.5)\n",
    "        display.clear_output(wait=True)\n",
    "        '''\n",
    "        \n",
    "        # forward the policy network and sample action according to the proba distribution\n",
    "        proba = model.predict(np.array([x]))\n",
    "        # print('Prediction: ' + str(proba))\n",
    "        # Variable proba is the probability prediction of how good UP_ACTION is for this frame.\n",
    "        # Then select UP_ACTION by proba percent. Easy way to still allow the other action.\n",
    "        # TODO: Mathematical reason for random number?\n",
    "        action = UP_ACTION if np.random.uniform() < proba else DOWN_ACTION\n",
    "        y = 1 if action == UP_ACTION else 0 # 0 and 1 are our labels\n",
    "    \n",
    "        # log the input and label to train later\n",
    "        x_train.append(x)\n",
    "        y_train.append(y)\n",
    "    \n",
    "        # do one step in our environment\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #print('Observation: ' + str(observation))\n",
    "        #print('Reward: ' + str(reward))\n",
    "        #print('Done: ' + str(done))\n",
    "        #print('Info: ' + str(info))\n",
    "        rewards.append(reward)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        # end of an episode\n",
    "        if done:\n",
    "            #print('At the end of episode', episode_nb, 'the total reward was :', reward_sum)\n",
    "            all_reward_sums = np.append(all_reward_sums, reward_sum)\n",
    "            \n",
    "            average_of_last = str(np.average(all_reward_sums[-epochs_before_saving:]))\n",
    "            #print('Average of current run: ' + str(average_of_last))\n",
    "            \n",
    "            # increment episode number\n",
    "            episode_nb += 1\n",
    "            \n",
    "            # training\n",
    "            # TODO: Is np.vstack is really necessary?\n",
    "            # TODO: Clarify the use of gamma in sample_weight=discount_rewards(rewards, gamma)\n",
    "            history = model.fit(x=np.vstack(x_train), y=np.vstack(y_train), verbose=0, sample_weight=discount_rewards(rewards, gamma))\n",
    "            accuracy = history.history['accuracy']                 \n",
    "                \n",
    "            if episode_nb % epochs_before_saving == 0:    \n",
    "                #model.save_weights('my_model_weights.h5')\n",
    "                print(str(configuration) + ' | ' + str(average_of_last) + ' | ' + str(accuracy) +  ' | ' + str(episode_nb) + '.h5')\n",
    "                \n",
    "            # Reinitialization\n",
    "            x_train, y_train, rewards = [],[],[]\n",
    "            observation = env.reset()\n",
    "            reward_sum = 0\n",
    "            prev_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_python_3_6",
   "language": "python",
   "name": "env_python_3_6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
